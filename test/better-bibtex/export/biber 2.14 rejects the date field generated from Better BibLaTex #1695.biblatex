
@online{ruderOverviewGradientDescent2016,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  date = {2016-01-19T14:20:00},
  url = {https://ruder.io/optimizing-gradient-descent/},
  urldate = {2020-11-24},
  abstract = {Gradient descent is the preferred way to optimize neural networks and many other machine learning algorithms but is often used as a black box. This post explores how many of the most popular gradient-based optimization algorithms such as Momentum, Adagrad, and Adam actually work.},
  langid = {english},
  organization = {{Sebastian Ruder}}
}


